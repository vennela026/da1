# -*- coding: utf-8 -*-
"""BeautifulSoup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z7HJBouLZbMxkRSEEAWpSlobwBFjY7KM

We start with an html element stored as a string in a variable
"""

!pip install bs4

"""https://www.crummy.com/software/BeautifulSoup/bs4/doc/"""

from bs4 import BeautifulSoup

# Sample HTML document
html_doc = """
<html>
<head><title>World's Population</title></head>
<body>
    <h1>Main Heading</h1>
    <p class="xyz">This is a paragraph.</p>
    <p class="content">Another paragraph with <a href="https://example.com">a link</a>.</p>

    <div id="container">
        <ul>
            <li class="item">Item 1</li>
            <li class="item">Item 2</li>
            <li class="item">Item 3</li>
        </ul>
    </div>

    <table>
        <tr><th>Name</th><th>Age</th></tr>
        <tr><td>Alice</td><td>25</td></tr>
        <tr><td>Bob</td><td>30</td></tr>
    </table>
</body>
</html>
"""

soup = BeautifulSoup(html_doc, "html.parser")
print(soup)

#Finding things via tags

# 1. **Basic Parsing**
print("Page Title:", soup.body.text)  # Extract title
#Limitation: Finds first occurence only

# 10. **Prettifying HTML Output**
x = BeautifulSoup("<html><body><span>This is first</span><p>This is second</p></body></html>","html.parser")
print("\nPrettified HTML:")
print(x.prettify())

# 2. **Finding Elements**
print("\nFind first paragraph:",( soup.find("p").text) ) # First <p> tag
print("Find all paragraphs:", soup.find_all("p"))  # All <p> tags
x = []
for i in soup.find_all("p"):
  x.append(i.text)
print(x)

# 3. **Using CSS Selectors**
print("\nUsing CSS Selectors:")
print(soup.select("p")[0].text)  # Select by tag name
print(soup.select(".xyz")[0].text)  # Select by class
print(soup.select("#container ul li")[1].text)  # Select second list item in <ul>

# 4. **Navigating the DOM**
print("\nNavigating the DOM:")
print(soup.body.h1.text)  # First <h1> tag inside <body>
print(soup.body.div.ul.li.text)  # First <li> inside <div> -> <ul>

# 5. **Extracting Attributes**
link = soup.find("a")
print(link.text)
print("\nExtracting Attributes:")
print("Link URL:", link["href"])  # Extract href attribute of <a>

# 6. **Modifying HTML**
print("\nModifying HTML:")
soup.find("h1").string = "Updated Heading"
print(soup.h1.text)

# 7. **Extracting Text**
print("\nExtracting All Text from Page:")
print(soup.get_text(separator='|'))

# 9. **Decomposing (Removing) Elements**
soup.find("h1").decompose()  # Remove <h1> tag
print("\nAfter Removing h1:", soup)

# 8. **Working with Tables**
table1 = soup.find("table")
rows = table1.find_all("tr")
print("\nTable Data:")
for row in rows:
   cols = row.find_all("td")
   print(row.text.split())
   if cols:
        print(f"Name: {cols[0].text}, Age: {cols[1].text}")

x='<html><th>Location</th><th>Population</th></html>'

x

print(type(x))

"""Importing Beautiful Soup, provided it's already installed"""

from bs4 import BeautifulSoup

soup=BeautifulSoup(x)
print(soup)
print(type(soup))

"""Finding First Occurence, soup.find is helping us find the first occurence"""

firstfind = soup.find('th')
print(firstfind)
print(type(firstfind))
print(firstfind.text)
print(type(firstfind.text))

"""Finding All occurences, soup.find_all is helping us find all occurences"""

allfind=soup.find_all('th')
print(allfind)
print(type(allfind))
print(allfind.text)#Error
print(type(allfind.text))#Error

for i in allfind:
  print(i)

for i in allfind:
  print(i.text)

headers=[]
for i in allfind:
  headers.append(i.text)

headers

"""By List Comprehension"""

headers = [i.text for i in allfind]
headers



"""# Beautiful Soup Quiz - Code Tracing

This notebook contains 10 questions focused on code tracing for Beautiful Soup. Each question includes code snippets, multiple-choice options, and the correct answer. Run the cells to see the correct answers.

### Question 1
```python

from bs4 import BeautifulSoup

html_doc = "<html><body><h1>Hello, World!</h1></body></html>"
soup = BeautifulSoup(html_doc, "html.parser")
print(soup.h1.text)
```

**Options:**
A) <h1>Hello, World!</h1>
B) Hello, World!
C) None
D) Error: AttributeError
"""

# Correct Answer
print("Correct Answer: B) Hello, World!")

"""### Question 2
```python

from bs4 import BeautifulSoup

html_doc = "<html><body><p class='info'>Python</p><p>Beautiful Soup</p></body></html>"
soup = BeautifulSoup(html_doc, "html.parser")
print(soup.find('p').text)
```

**Options:**
A) Python
B) Beautiful Soup
C) <p class='info'>Python</p>
D) None
"""

# Correct Answer
print("Correct Answer: A) Python")

"""### Question 3
```python

from bs4 import BeautifulSoup

html_doc = '''
<html>
  <body>
    <p class='content'>Paragraph 1</p>
    <p>Paragraph 2</p>
    <p class='content'>Paragraph 3</p>
  </body>
</html>
'''
soup = BeautifulSoup(html_doc, "html.parser")
result = soup.find_all('p', class_='content')
for item in result:
    print(item.text)
```

**Options:**
A) Paragraph 1
B) Paragraph 1 and Paragraph 3
C) Paragraph 2
D) None
"""

# Correct Answer
print("Correct Answer: B) Paragraph 1 and Paragraph 3")

"""### Question 4
```python

from bs4 import BeautifulSoup

html_doc = "<html><a href='https://example.com'>Visit Example</a></html>"
soup = BeautifulSoup(html_doc, "html.parser")
print(soup.a['href'])
```

**Options:**
A) https://example.com
B) Visit Example
C) <a href='https://example.com'>Visit Example</a>
D) None
"""

# Correct Answer
print("Correct Answer: A) https://example.com")

"""### Question 5
```python

from bs4 import BeautifulSoup

html_doc = '''
<html>
  <body>
    <div>
      <h1>Title</h1>
      <p>Description</p>
    </div>
  </body>
</html>
'''
soup = BeautifulSoup(html_doc, "html.parser")
print(soup.div.h1.text)
```

**Options:**
A) Title
B) Description
C) <h1>Title</h1>
D) None
"""

# Correct Answer
print("Correct Answer: A) Title")

"""### Question 6
```python

from bs4 import BeautifulSoup

html_doc = '''
<html>
  <body>
    <ul>
      <li>Item 1</li>
      <li>Item 2</li>
      <li>Item 3</li>
    </ul>
  </body>
</html>
'''
soup = BeautifulSoup(html_doc, "html.parser")
items = soup.find_all('li')
print(len(items))
```

**Options:**
A) 1
B) 2
C) 3
D) 0
"""

# Correct Answer
print("Correct Answer: C) 3")

"""### Question 7
```python

from bs4 import BeautifulSoup

html_doc = '''
<html>
  <body>
    <div id="main">
      <p class="text">First</p>
      <p class="text">Second</p>
    </div>
  </body>
</html>
'''
soup = BeautifulSoup(html_doc, "html.parser")
result = soup.select('#main .text')
for item in result:
    print(item.text)
```

**Options:**
A) First
B) First and Second
C) Second
D) None
"""

# Correct Answer
print("Correct Answer: B) First and Second")

"""### Question 8
```python

from bs4 import BeautifulSoup

html_doc = "<html><body><h1>Hello</h1><p>World</p></body></html>"
soup = BeautifulSoup(html_doc, "html.parser")
print(soup.get_text())
```

**Options:**
A) Hello World
B) Hello\nWorld
C) <h1>Hello</h1><p>World</p>
D) None
"""

# Correct Answer
print("Correct Answer: B) Hello\nWorld")

"""### Question 9
```python

from bs4 import BeautifulSoup

html_doc = '''
<html>
  <body>
    <img src="image1.png" alt="First Image">
    <img src="image2.png" alt="Second Image">
  </body>
</html>
'''
soup = BeautifulSoup(html_doc, "html.parser")
image = soup.find('img', alt='Second Image')
print(image['src'])
```

**Options:**
A) image1.png
B) image2.png
C) <img src='image2.png' alt='Second Image'>
D) None
"""

# Correct Answer
print("Correct Answer: B) image2.png")

"""### Question 10
```python

from bs4 import BeautifulSoup

html_doc = '''
<html>
  <body>
    <p class="info detail">Content 1</p>
    <p class="info">Content 2</p>
  </body>
</html>
'''
soup = BeautifulSoup(html_doc, "html.parser")
result = soup.find_all('p', class_='info')
for item in result:
    print(item.text)
```

**Options:**
A) Content 1
B) Content 2
C) Content 1 and Content 2
D) None
"""

# Correct Answer
print("Correct Answer: C) Content 1 and Content 2")

"""**One Small Project**

Beautiful Soup is a Python library used for web scraping purposes. It allows you to extract and manipulate data from HTML and XML documents. Here's an example to demonstrate how to use Beautiful Soup to scrape and process a webpage:

**Example: Scraping Titles of Blog Posts**
"""

from bs4 import BeautifulSoup
import requests

# URL of the webpage you want to scrape (using a real blog URL for example)
url = 'https://realpython.com/'

# Send an HTTP request to the webpage
response = requests.get(url)

# Parse the HTML content of the page
soup = BeautifulSoup(response.text, 'html.parser')

# Extract all the blog post titles (example: assuming titles are in <h2> tags with class 'card-title')
titles = soup.find_all('h2', class_='card-title')

# Print the extracted titles
print("Blog Post Titles:")
for i, title in enumerate(titles, 1):
    print(f"{i}. {title.text.strip()}")

"""Step 1: Import Libraries
The first step is to import the necessary libraries: BeautifulSoup for parsing the HTML content of the webpage and requests for making HTTP requests. These libraries allow us to retrieve the webpage and extract specific elements from its structure.

Step 2: Specify the Target URL
You define the URL of the webpage you want to scrape. In this project, the target is a blog site, such as "https://realpython.com/". This URL serves as the entry point for fetching the content that you want to analyze.

Step 3: Send an HTTP Request
An HTTP GET request is sent to the specified URL using the requests library. The server responds with the HTML content of the webpage, which we can then process. This step ensures you have access to the page's raw content.

Step 4: Parse the HTML Content
The HTML content from the response is passed to BeautifulSoup, which parses it and converts it into a structured format that can be navigated like a tree. This enables you to efficiently locate and extract specific elements (e.g., titles, links, or text) from the webpage.

Step 5: Locate Blog Post Titles
BeautifulSoup's find_all method is used to search for specific tags (e.g., <h2>) and their associated class attributes (e.g., class='card-title'). This method retrieves all the matching elements, which in this case are the blog post titles. The find_all function returns a list containing these elements.

Step 6: Loop Through Titles
The list of blog post titles is iterated using a loop. For each title, the text content is extracted using the .text attribute of the tag, and unnecessary whitespace is removed using the .strip() method. The loop prints each title alongside its position in the list.

Step 7: Display the Extracted Titles
Finally, the extracted blog titles are displayed as a numbered list. Each title is printed in a clean format, making it easy to see the output of the project. This step concludes the task, giving you a clear view of the blog posts retrieved from the webpage.

This step-by-step process demonstrates the basic workflow of web scraping: accessing a webpage, parsing its structure, and extracting specific data in a meaningful way.
"""